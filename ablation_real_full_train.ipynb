{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ablation_real_full_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "599d61813a9145e4912cd4757ce1f7d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_005785e0fe3d43a9855c96cea82ada4b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9c21e6463c9c413b8d3588c0269feb6f",
              "IPY_MODEL_a81475baa35a487499eeab872c5d6112",
              "IPY_MODEL_3e4ec84f89d84a1895934fd0805ba1bd"
            ]
          }
        },
        "005785e0fe3d43a9855c96cea82ada4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c21e6463c9c413b8d3588c0269feb6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9a14d00141794583a584a18420c24400",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03d891f81ccb4a83ab8dcab95bc6659c"
          }
        },
        "a81475baa35a487499eeab872c5d6112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_50e0a0eaebee45a78f3039a12b3a2bca",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1399,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1399,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_393bb7e4e4e3484692dc6d82a531f343"
          }
        },
        "3e4ec84f89d84a1895934fd0805ba1bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_df963e0eca544f8ebd88fe8499421bd9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1399/1399 [00:00&lt;00:00, 6519.46it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a229a058466a4ab4afc374cfa45ad3bd"
          }
        },
        "9a14d00141794583a584a18420c24400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03d891f81ccb4a83ab8dcab95bc6659c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "50e0a0eaebee45a78f3039a12b3a2bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "393bb7e4e4e3484692dc6d82a531f343": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df963e0eca544f8ebd88fe8499421bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a229a058466a4ab4afc374cfa45ad3bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9911a52a6b8f48ebb0670a8b7d528dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0ea2f28111de4e479eb3f8e5f4bfaba8",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_59546349d30a426cbf352dc6bbc762a9",
              "IPY_MODEL_df8207d6c60641e6a2ac165157add297",
              "IPY_MODEL_b2664c48eb714d3a9469bd71de0ef391"
            ]
          }
        },
        "0ea2f28111de4e479eb3f8e5f4bfaba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59546349d30a426cbf352dc6bbc762a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4e5eed5f829c461dba48b83ef9bac8bc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9754375d3c4c4f1d88b29d2d60e903b0"
          }
        },
        "df8207d6c60641e6a2ac165157add297": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e16ed94912ff4f1e856f4cebb02ea752",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 13990,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13990,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b5cccdca19c41f8a6fdcbacd40c8e22"
          }
        },
        "b2664c48eb714d3a9469bd71de0ef391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_40cdd8c0de7346cc948ad40f789de694",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13990/13990 [00:00&lt;00:00, 19242.41it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c2223da80714a15b867048635cce2c9"
          }
        },
        "4e5eed5f829c461dba48b83ef9bac8bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9754375d3c4c4f1d88b29d2d60e903b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e16ed94912ff4f1e856f4cebb02ea752": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b5cccdca19c41f8a6fdcbacd40c8e22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40cdd8c0de7346cc948ad40f789de694": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c2223da80714a15b867048635cce2c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vfrantc/dehaze_detect/blob/main/ablation_real_full_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7iTEHLw-LJh",
        "outputId": "a74f4dc7-10bb-4e41-f99b-36eed2771dcc"
      },
      "source": [
        "!pip install git+https://github.com/vfrantc/Pytorch-Quaternion-Neural-Networks.git\n",
        "!pip install --upgrade git+https://github.com/Lyken17/pytorch-OpCounter.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/vfrantc/Pytorch-Quaternion-Neural-Networks.git\n",
            "  Cloning https://github.com/vfrantc/Pytorch-Quaternion-Neural-Networks.git to /tmp/pip-req-build-ye7p8tzp\n",
            "  Running command git clone -q https://github.com/vfrantc/Pytorch-Quaternion-Neural-Networks.git /tmp/pip-req-build-ye7p8tzp\n",
            "Building wheels for collected packages: Pytorch-QNN\n",
            "  Building wheel for Pytorch-QNN (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pytorch-QNN: filename=Pytorch_QNN-1-py3-none-any.whl size=21517 sha256=e18a07cb58a70476e06bf921b79c0d8512311a5b68c2aa2a68cefd6e1e0f3a66\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-w1a130fp/wheels/b0/e6/5e/12647ec42981f7a546d45f42c9103af281944d0f75f91c1c89\n",
            "Successfully built Pytorch-QNN\n",
            "Installing collected packages: Pytorch-QNN\n",
            "Successfully installed Pytorch-QNN-1\n",
            "Collecting git+https://github.com/Lyken17/pytorch-OpCounter.git\n",
            "  Cloning https://github.com/Lyken17/pytorch-OpCounter.git to /tmp/pip-req-build-xy6qh_j2\n",
            "  Running command git clone -q https://github.com/Lyken17/pytorch-OpCounter.git /tmp/pip-req-build-xy6qh_j2\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from thop==0.0.5-2111260904) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->thop==0.0.5-2111260904) (3.10.0.2)\n",
            "Building wheels for collected packages: thop\n",
            "  Building wheel for thop (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thop: filename=thop-0.0.5.post2111260904-py3-none-any.whl size=14779 sha256=a131731a1e2df6d5b56bd83c4f6b5406a1f54f50b44c62313037d409ecdf7b74\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r5b7e927/wheels/aa/e7/25/c280567dc2e6a1f3aadf802f16129960793c1bc889dbd8ee4e\n",
            "Successfully built thop\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.0.5.post2111260904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKaSMHLCGy8x",
        "outputId": "2aa4ff60-1941-485d-bd0a-cab9e5ec8913"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Nov 26 09:04:26 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    30W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61n1iDXE_lak"
      },
      "source": [
        "import os\n",
        "import datetime\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from scipy import signal\n",
        "from torchvision.utils import make_grid\n",
        "import numpy.random as random\n",
        "import torchsummary\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from core_qnn.quaternion_layers import QuaternionTransposeConv\n",
        "from core_qnn.quaternion_layers import QuaternionConv\n",
        "from core_qnn.quaternion_layers import QuaternionLinearAutograd\n",
        "from core_qnn.quaternion_layers import QuaternionLinear\n",
        "from core_qnn.quaternion_ops import check_input\n",
        "from core_qnn.quaternion_ops import get_r, get_i, get_j, get_k"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xyHJyTb29fby",
        "outputId": "b3e0e599-49e4-4633-a969-3d210fa8fd54"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Y9Y2MJewK-"
      },
      "source": [
        "!cp /content/drive/MyDrive/dataset2.zip .\n",
        "!unzip -q dataset2.zip\n",
        "!mv I-HAZE/hazy .\n",
        "!rm -rf I-HAZE\n",
        "!mv hazy I-HAZE \n",
        "!mv O-HAZE/hazy . \n",
        "!rm -rf O-HAZE \n",
        "!mv hazy O-HAZE"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50TdJBxbewVY"
      },
      "source": [
        "!mv ITS/clear ITS/gt"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "599d61813a9145e4912cd4757ce1f7d9",
            "005785e0fe3d43a9855c96cea82ada4b",
            "9c21e6463c9c413b8d3588c0269feb6f",
            "a81475baa35a487499eeab872c5d6112",
            "3e4ec84f89d84a1895934fd0805ba1bd",
            "9a14d00141794583a584a18420c24400",
            "03d891f81ccb4a83ab8dcab95bc6659c",
            "50e0a0eaebee45a78f3039a12b3a2bca",
            "393bb7e4e4e3484692dc6d82a531f343",
            "df963e0eca544f8ebd88fe8499421bd9",
            "a229a058466a4ab4afc374cfa45ad3bd",
            "9911a52a6b8f48ebb0670a8b7d528dda",
            "0ea2f28111de4e479eb3f8e5f4bfaba8",
            "59546349d30a426cbf352dc6bbc762a9",
            "df8207d6c60641e6a2ac165157add297",
            "b2664c48eb714d3a9469bd71de0ef391",
            "4e5eed5f829c461dba48b83ef9bac8bc",
            "9754375d3c4c4f1d88b29d2d60e903b0",
            "e16ed94912ff4f1e856f4cebb02ea752",
            "7b5cccdca19c41f8a6fdcbacd40c8e22",
            "40cdd8c0de7346cc948ad40f789de694",
            "1c2223da80714a15b867048635cce2c9"
          ]
        },
        "id": "oIkHiN_dewaL",
        "outputId": "c4f0e287-a504-4899-d889-3dec5eacdfa9"
      },
      "source": [
        "from glob import glob \n",
        "from tqdm.notebook import tqdm\n",
        "import shutil\n",
        "\n",
        "for fname in tqdm(glob('ITS/gt/*')):\n",
        "  out = os.path.join(os.path.dirname(fname), 't'+os.path.basename(fname))\n",
        "  shutil.move(fname, out)\n",
        "\n",
        "for fname in tqdm(glob('ITS/hazy/*')):\n",
        "  out = os.path.join(os.path.dirname(fname), 't'+os.path.basename(fname))\n",
        "  shutil.move(fname, out)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "599d61813a9145e4912cd4757ce1f7d9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1399 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9911a52a6b8f48ebb0670a8b7d528dda",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/13990 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWLKXcaifAKw"
      },
      "source": [
        "!cp -r ITS/gt OTS/ \n",
        "!cp -r ITS/hazy OTS/"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhPH1JMGfAOH"
      },
      "source": [
        "!cp /content/drive/MyDrive/SOTS.zip .\n",
        "!unzip -q SOTS.zip"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z09ZnDXsfARJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-XgCI_bfATd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiIxeiaLeweT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f45Irh1x_-0s"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK0kFuR__sQs"
      },
      "source": [
        "IMG_EXTENSIONS = [\n",
        "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
        "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n",
        "]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0FK4KkE_sS0"
      },
      "source": [
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5vIZRvi_sVc"
      },
      "source": [
        "def make_dataset(dir):\n",
        "    images = []\n",
        "    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
        "\n",
        "    for root, _, fnames in sorted(os.walk(dir)):\n",
        "        for fname in fnames:\n",
        "            if is_image_file(fname):\n",
        "                path = os.path.join(root, fname)\n",
        "                images.append(path)\n",
        "\n",
        "    return images"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYRGhIv5_sXq"
      },
      "source": [
        "def edge_compute(x):\n",
        "    x_diffx = torch.abs(x[:,:,1:] - x[:,:,:-1])\n",
        "    x_diffy = torch.abs(x[:,1:,:] - x[:,:-1,:])\n",
        "\n",
        "    y = x.new(x.size())\n",
        "    y.fill_(0)\n",
        "    y[:,:,1:] += x_diffx\n",
        "    y[:,:,:-1] += x_diffx\n",
        "    y[:,1:,:] += x_diffy\n",
        "    y[:,:-1,:] += x_diffy\n",
        "    y = torch.sum(y,0,keepdim=True)/3\n",
        "    y /= 4\n",
        "    return y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTjmsAdO_saf"
      },
      "source": [
        "def batch_edge_compute(x):\n",
        "    x_diffx = torch.abs(x[:,:,:,1:] - x[:,:,:,:-1])\n",
        "    x_diffy = torch.abs(x[:,:,1:,:] - x[:,:,:-1,:])\n",
        "\n",
        "    y = x.new(x.size())\n",
        "    y.fill_(0)\n",
        "    y[:,:,:,1:] += x_diffx\n",
        "    y[:,:,:,:-1] += x_diffx\n",
        "    y[:,:,1:,:] += x_diffy\n",
        "    y[:,:,:-1,:] += x_diffy\n",
        "    y = torch.sum(y,1,keepdim=True)/3\n",
        "    y /= 4\n",
        "    return y"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kntHEBEJ_sdK"
      },
      "source": [
        "# Converts a Tensor into an image array (numpy)\n",
        "# |imtype|: the desired type of the converted numpy array\n",
        "def tensor2im(input_image, imtype=np.uint8):\n",
        "    if isinstance(input_image, torch.Tensor):\n",
        "        image_tensor = input_image.data\n",
        "    else:\n",
        "        return input_image\n",
        "    image_numpy = image_tensor[0].cpu().float().numpy()\n",
        "    if image_numpy.shape[0] == 1:\n",
        "        image_numpy = np.tile(image_numpy, (3, 1, 1))\n",
        "    image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "    image_numpy = image_numpy.clip(0, 255)\n",
        "    return image_numpy.astype(imtype)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6bTUFbbAPIM"
      },
      "source": [
        "def tensor2imgrid(input_image):\n",
        "    im_grid = make_grid(input_image[:4, ...], nrow=2, normalize=True, range=(-128, 128))\n",
        "    return im_grid\n",
        "    # ndarr = im_grid.mul(255).clamp(0, 255).byte().permute(1, 2, 0).cpu().numpy()\n",
        "    # im = Image.fromarray(ndarr)\n",
        "    # return im"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hxIDAhPARGR"
      },
      "source": [
        "def diagnose_network(net, name='network'):\n",
        "    mean = 0.0\n",
        "    count = 0\n",
        "    for param in net.parameters():\n",
        "        if param.grad is not None:\n",
        "            mean += torch.mean(torch.abs(param.grad.data))\n",
        "            count += 1\n",
        "    if count > 0:\n",
        "        mean = mean / count\n",
        "    print(name)\n",
        "    print(mean)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzxNa4eRARIe"
      },
      "source": [
        "def save_image(image_numpy, image_path):\n",
        "    image_pil = Image.fromarray(image_numpy)\n",
        "    image_pil.save(image_path)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1WntY9tARKc"
      },
      "source": [
        "def print_numpy(x, val=True, shp=False):\n",
        "    x = x.astype(np.float64)\n",
        "    if shp:\n",
        "        print('shape,', x.shape)\n",
        "    if val:\n",
        "        x = x.flatten()\n",
        "        print('mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f' % (\n",
        "            np.mean(x), np.min(x), np.max(x), np.median(x), np.std(x)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxpi18nTAPKd"
      },
      "source": [
        "def mkdirs(paths):\n",
        "    if isinstance(paths, list) and not isinstance(paths, str):\n",
        "        for path in paths:\n",
        "            mkdir(path)\n",
        "    else:\n",
        "        mkdir(paths)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9VT_UpUAPNR"
      },
      "source": [
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubYy6GYiAPPT"
      },
      "source": [
        "def fspecial_gauss(size, sigma):\n",
        "    \"\"\"Function to mimic the 'fspecial' gaussian MATLAB function\n",
        "    \"\"\"\n",
        "    x, y = np.mgrid[-size // 2 + 1:size // 2 + 1, -size // 2 + 1:size // 2 + 1]\n",
        "    g = np.exp(-((x ** 2 + y ** 2) / (2.0 * sigma ** 2)))\n",
        "    return g / g.sum()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBzm_GbM_sfv"
      },
      "source": [
        "def filter2(x, kernel, mode='same'):\n",
        "    return signal.convolve2d(x, np.rot90(kernel, 2), mode=mode)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6rFv5jZAdzP"
      },
      "source": [
        "def ssim(img1, img2, cs_map=False):\n",
        "    \"\"\"Return the Structural Similarity Map corresponding to input images img1\n",
        "    and img2 (images are assumed to be uint8)\n",
        "\n",
        "    This function attempts to mimic precisely the functionality of ssim.m a\n",
        "    MATLAB provided by the author's of SSIM\n",
        "    https://ece.uwaterloo.ca/~z70wang/research/ssim/ssim_index.m\n",
        "    \"\"\"\n",
        "    img1 = img1.astype(np.float64)\n",
        "    img2 = img2.astype(np.float64)\n",
        "    size = 11\n",
        "    sigma = 1.5\n",
        "    window = fspecial_gauss(size, sigma)\n",
        "    K1 = 0.01\n",
        "    K2 = 0.03\n",
        "    L = 255  # bitdepth of image\n",
        "    C1 = (K1 * L) ** 2\n",
        "    C2 = (K2 * L) ** 2\n",
        "    mu1 = filter2(img1, window, mode='valid')\n",
        "    mu2 = filter2(img2, window, mode='valid')\n",
        "    mu1_sq = mu1 * mu1\n",
        "    mu2_sq = mu2 * mu2\n",
        "    mu1_mu2 = mu1 * mu2\n",
        "    sigma1_sq = filter2(img1 * img1, window, mode='valid') - mu1_sq\n",
        "    sigma2_sq = filter2(img2 * img2, window, mode='valid') - mu2_sq\n",
        "    sigma12 = filter2(img1 * img2, window, mode='valid') - mu1_mu2\n",
        "    if cs_map:\n",
        "        return np.mean(np.mean((((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n",
        "                                                             (sigma1_sq + sigma2_sq + C2)),\n",
        "                (2.0 * sigma12 + C2) / (sigma1_sq + sigma2_sq + C2))))\n",
        "    else:\n",
        "        return np.mean(np.mean(((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n",
        "                                                            (sigma1_sq + sigma2_sq + C2))))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt-9wBbYAd1n"
      },
      "source": [
        "class MovingAvg(object):\n",
        "    def __init__(self, pool_size=100):\n",
        "        from queue import Queue\n",
        "        self.pool = Queue(maxsize=pool_size)\n",
        "        self.sum = 0\n",
        "        self.curr_pool_size = 0\n",
        "        self.pool_size = pool_size\n",
        "\n",
        "    def set_curr_val(self, val):\n",
        "        if not self.pool.full():\n",
        "            self.curr_pool_size += 1\n",
        "            self.pool.put_nowait(val)\n",
        "        else:\n",
        "            last_first_val = self.pool.get_nowait()\n",
        "            self.pool.put_nowait(val)\n",
        "            self.sum -= last_first_val\n",
        "\n",
        "        self.sum += val\n",
        "        return self.sum / self.curr_pool_size\n",
        "\n",
        "    def reset(self):\n",
        "        from queue import Queue\n",
        "        self.pool = Queue(maxsize=self.pool_size)\n",
        "        self.sum = 0\n",
        "        self.curr_pool_size = 0"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwVJR_-VAmVz"
      },
      "source": [
        "# Folder loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzoURrapAd3z"
      },
      "source": [
        "class FolderLoader(object):\n",
        "    def __init__(self, fold_path):\n",
        "        super(FolderLoader, self).__init__()\n",
        "        self.fold_path = fold_path\n",
        "        self.img_paths = make_dataset(self.fold_path)\n",
        "        self.img_names = [os.path.basename(x) for x in self.img_paths]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.img_paths[index])#.convert('RGB')\n",
        "        return self.img_names[index], img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_names)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhteEF8wBR6_"
      },
      "source": [
        "def pil_loader(img_path):\n",
        "    return Image.open(img_path).convert(\"RGB\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfuYcN_OBSFh"
      },
      "source": [
        "class ImagePairPrefixFolder(Dataset):\n",
        "    def __init__(self, input_folder, gt_folder, max_img_size=0, size_unit=1, force_rgb=False):\n",
        "        super(ImagePairPrefixFolder, self).__init__()\n",
        "\n",
        "        self.gt_loader = FolderLoader(gt_folder)\n",
        "        # build the map from image name to index\n",
        "        self.gt_map = dict()\n",
        "        for idx, img_name in enumerate(self.gt_loader.img_names):\n",
        "            self.gt_map[os.path.splitext(img_name)[0].split('_')[0]] = idx\n",
        "\n",
        "        self.input_loader = FolderLoader(input_folder)\n",
        "        assert all([os.path.splitext(x)[0].split('_')[0] in self.gt_map for x in self.input_loader.img_names]), \\\n",
        "                'cannot find corresponding gt names'\n",
        "\n",
        "\n",
        "        self.input_folder = input_folder\n",
        "        self.gt_folder = gt_folder\n",
        "        self.max_img_size = max_img_size\n",
        "        self.size_unit = size_unit\n",
        "        self.force_rgb = force_rgb\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        input_name, input_img = self.input_loader[index]\n",
        "        input_basename = os.path.splitext(input_name)[0].split('_')[0]\n",
        "        gt_idx = self.gt_map[input_basename]\n",
        "\n",
        "        gt_name, gt_img = self.gt_loader[gt_idx]\n",
        "        if self.force_rgb:\n",
        "            input_img = input_img.convert('RGB')\n",
        "            gt_img = gt_img.convert('RGB')\n",
        "        im_w, im_h = input_img.size\n",
        "        gt_w, gt_h = gt_img.size\n",
        "        assert im_w==gt_w and im_h==gt_h, 'input image and gt image size not match'\n",
        "\n",
        "        im_w, im_h = input_img.size\n",
        "        if 0 < self.max_img_size < max(im_w, im_h):\n",
        "            if im_w < im_h:\n",
        "                out_h = int(self.max_img_size) // self.size_unit * self.size_unit\n",
        "                out_w = int(im_w / im_h * out_h) // self.size_unit * self.size_unit\n",
        "            else:\n",
        "                out_w = int(self.max_img_size) // self.size_unit * self.size_unit\n",
        "                out_h = int(im_h / im_w * out_w) // self.size_unit * self.size_unit\n",
        "        else:\n",
        "            out_w = im_w // self.size_unit * self.size_unit\n",
        "            out_h = im_h // self.size_unit * self.size_unit\n",
        "\n",
        "        if im_w != out_w or im_h != out_h:\n",
        "            input_img = input_img.resize((out_w, out_h), Image.BILINEAR)\n",
        "            gt_img = gt_img.resize((out_w, out_h), Image.BILINEAR)\n",
        "\n",
        "        im_w, im_h = input_img.size\n",
        "\n",
        "        \n",
        "        #input_img = np.array(input_img).astype('float')\n",
        "        #input_img = np.array(input_img)\n",
        "\n",
        "        input_img = np.array(input_img)\n",
        "        #gray = (input_img[:, :, 0] + input_img[:, :, 1] + input_img[:, :, 2]) / 3\n",
        "        gray = cv2.cvtColor(input_img, cv2.COLOR_RGB2GRAY)\n",
        "        #gray = cv2.imread(os.path.join('ds/train_general/trans/', input_name.replace('.jpg', '.png')), 0)\n",
        "        #gray = cv2.resize(gray, (out_w, out_h))\n",
        "        \n",
        "        input_img = np.dstack([gray[:, :, np.newaxis], input_img])\n",
        "        input_img = input_img.astype('float')\n",
        "\n",
        "\n",
        "        gt_img = np.array(gt_img)\n",
        "        #gray = cv2.cvtColor(gt_img, cv2.COLOR_RGB2GRAY)\n",
        "        gray = (gt_img[:, :, 0] + gt_img[:, :, 1] + gt_img[:, :, 2]) / 3\n",
        "        gt_img = np.dstack([gray[:, :, np.newaxis], gt_img])\n",
        "        gt_img = gt_img.astype('float')\n",
        "        if len(input_img.shape) == 2:\n",
        "            input_img = input_img[:, :, np.newaxis]\n",
        "        if len(gt_img.shape) == 2:\n",
        "            gt_img = gt_img[:, :, np.newaxis]\n",
        "\n",
        "        input_img = input_img\n",
        "        gt_img = gt_img\n",
        "        return {'input_img': input_img, 'gt_img': gt_img,  'input_h': im_h, \"input_w\": im_w}\n",
        "\n",
        "    def get_input_info(self, index):\n",
        "        image_name = os.path.splitext(self.input_loader.img_names[index])[0]\n",
        "        return self.input_loader, image_name\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_loader)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cji84_O5BZX0"
      },
      "source": [
        "def var_custom_collate(batch):\n",
        "    min_h, min_w = 10000, 10000\n",
        "    for item in batch:\n",
        "        min_h = min(min_h, item['input_h'])\n",
        "        min_w = min(min_w, item['input_w'])\n",
        "    inc = 1 if len(batch[0]['input_img'].shape)==2 else batch[0]['input_img'].shape[2]\n",
        "    batch_input_images = torch.Tensor(len(batch), 4, min_h, min_w)\n",
        "    batch_gt_images = torch.Tensor(len(batch), 4, min_h, min_w)\n",
        "\n",
        "    for idx, item in enumerate(batch):\n",
        "        off_y = 0 if item['input_h']==min_h else random.randint(0, item['input_h'] - min_h)\n",
        "        off_x = 0 if item['input_w']==min_w else random.randint(0, item['input_w'] - min_w)\n",
        "        crop_input_img = item['input_img'][off_y:off_y + min_h, off_x:off_x + min_w, :]\n",
        "        crop_gt_img = item['gt_img'][off_y:off_y + min_h, off_x:off_x + min_w, :]\n",
        "        batch_input_images[idx] = torch.from_numpy(crop_input_img.transpose((2, 0, 1))) - 128\n",
        "        batch_gt_images[idx] = torch.from_numpy(crop_gt_img.transpose((2, 0, 1)))\n",
        "\n",
        "\n",
        "    batch_input_edges = batch_edge_compute(batch_input_images) - 128\n",
        "    return batch_input_images, batch_input_edges,  batch_gt_images"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqgIOJoqArjA"
      },
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdjHV5g2AqnB"
      },
      "source": [
        "class ShareSepConv(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super(ShareSepConv, self).__init__()\n",
        "        assert kernel_size % 2 == 1, 'kernel size should be odd'\n",
        "        self.padding = (kernel_size - 1)//2\n",
        "        weight_tensor = torch.zeros(1, 1, kernel_size, kernel_size)\n",
        "        weight_tensor[0, 0, (kernel_size-1)//2, (kernel_size-1)//2] = 1\n",
        "        self.weight = nn.Parameter(weight_tensor)\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        inc = x.size(1)\n",
        "        expand_weight = self.weight.expand(inc, 1, self.kernel_size, self.kernel_size).contiguous()\n",
        "        return F.conv2d(x, expand_weight,\n",
        "                        None, 1, self.padding, 1, inc)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duT8AB12Aqrq"
      },
      "source": [
        "class SmoothDilatedResidualBlock(nn.Module):\n",
        "    def __init__(self, channel_num, dilation=1, group=1):\n",
        "        super(SmoothDilatedResidualBlock, self).__init__()\n",
        "        self.pre_conv1 = ShareSepConv(dilation*2-1)\n",
        "        self.conv1 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group, bias=False)\n",
        "        self.norm1 = nn.InstanceNorm2d(channel_num, affine=True)\n",
        "        self.pre_conv2 = ShareSepConv(dilation*2-1)\n",
        "        self.conv2 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group, bias=False)\n",
        "        self.norm2 = nn.InstanceNorm2d(channel_num, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.relu(self.norm1(self.conv1(self.pre_conv1(x))))\n",
        "        y = self.norm2(self.conv2(self.pre_conv2(y)))\n",
        "        return F.relu(x+y)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkFC-x0PAqui"
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, channel_num, dilation=1, group=1):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group, bias=False)\n",
        "        self.norm1 = nn.InstanceNorm2d(channel_num, affine=True)\n",
        "        self.conv2 = nn.Conv2d(channel_num, channel_num, 3, 1, padding=dilation, dilation=dilation, groups=group, bias=False)\n",
        "        self.norm2 = nn.InstanceNorm2d(channel_num, affine=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.relu(self.norm1(self.conv1(x)))\n",
        "        y = self.norm2(self.conv2(y))\n",
        "        return F.relu(x+y)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb2b4XfzBDgS"
      },
      "source": [
        "class GCANet(nn.Module):\n",
        "    def __init__(self, in_c=4, out_c=3, only_residual=True):\n",
        "        super(GCANet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_c, 64, 3, stride=1, dilation=1, padding=1, bias=False)\n",
        "        self.norm1 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv2 = nn.Conv2d(64, 64, 3, stride=1, dilation=1, padding=1, bias=False)\n",
        "        self.norm2 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.conv3 = nn.Conv2d(64, 64, 3, stride=2, dilation=1, padding=1, bias=False)\n",
        "        self.norm3 = nn.InstanceNorm2d(64, affine=True)\n",
        "\n",
        "        self.res1 = SmoothDilatedResidualBlock(64, dilation=2)\n",
        "        self.res2 = SmoothDilatedResidualBlock(64, dilation=2)\n",
        "        self.res3 = SmoothDilatedResidualBlock(64, dilation=2)\n",
        "        self.res4 = SmoothDilatedResidualBlock(64, dilation=4)\n",
        "        self.res5 = SmoothDilatedResidualBlock(64, dilation=4)\n",
        "        self.res6 = SmoothDilatedResidualBlock(64, dilation=4)\n",
        "        self.res7 = ResidualBlock(64, dilation=1)\n",
        "\n",
        "        self.gate = nn.Conv2d(64 * 3, 64, 1, 1, 1, bias=True)\n",
        "\n",
        "        self.deconv3 = nn.ConvTranspose2d(64, 64, 4, stride=2, dilation=1, padding=3, output_padding=0)\n",
        "        self.norm4 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv2 = nn.ConvTranspose2d(2 * 64, 64, 3, stride=1, dilation=1, padding=1)\n",
        "        self.norm5 = nn.InstanceNorm2d(64, affine=True)\n",
        "        self.deconv1 = nn.Conv2d(64, out_c, 1, stride=1, dilation=1, padding=0, bias=True)\n",
        "        self.only_residual = only_residual\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = F.relu(self.norm1(self.conv1(x)))\n",
        "        y0 = F.relu(self.norm2(self.conv2(y)))\n",
        "        y1 = F.relu(self.norm3(self.conv3(y0)))\n",
        "\n",
        "        y = self.res1(y1)\n",
        "        y = self.res2(y)\n",
        "        y = self.res3(y)\n",
        "        y2 = self.res4(y)\n",
        "        y = self.res5(y2)\n",
        "        y = self.res6(y)\n",
        "        y3 = self.res7(y)\n",
        "\n",
        "\n",
        "        # r = torch.cat((get_r(y1), get_r(y2), get_r(y3)), dim=1)\n",
        "        # i = torch.cat((get_i(y1), get_i(y2), get_i(y3)), dim=1)\n",
        "        # j = torch.cat((get_j(y1), get_j(y2), get_j(y3)), dim=1)\n",
        "        # k = torch.cat((get_k(y1), get_k(y2), get_k(y3)), dim=1)\n",
        "\n",
        "        gated_y = self.gate(torch.cat((y1, y2, y3), dim=1))\n",
        "\n",
        "        y = F.relu(self.norm4(self.deconv3(gated_y)))   \n",
        "        # r = torch.cat((get_r(y0), get_r(y)), dim=1)\n",
        "        # i = torch.cat((get_i(y0), get_i(y)), dim=1)\n",
        "        # j = torch.cat((get_j(y0), get_j(y)), dim=1)\n",
        "        # k = torch.cat((get_k(y0), get_k(y)), dim=1)\n",
        "        # print('y0: ', y0.size())\n",
        "        # print('y: ', y.size())\n",
        "\n",
        "        y = torch.cat((y0, y), dim=1)     \n",
        "        y = F.relu(self.norm5(self.deconv2(y)))\n",
        "        if self.only_residual:\n",
        "            y = self.deconv1(y)\n",
        "        else:\n",
        "            y = F.relu(self.deconv1(y))\n",
        "\n",
        "        return y"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C88T2V8ilKrK"
      },
      "source": [
        "# Perceptual loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__o0aIqclZqA"
      },
      "source": [
        "# from torchvision.models import vgg16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpKENP0vmHP6"
      },
      "source": [
        "# class LossNetwork(torch.nn.Module):\n",
        "#     def __init__(self, vgg_model):\n",
        "#         super(LossNetwork, self).__init__()\n",
        "#         self.vgg_layers = vgg_model\n",
        "#         self.layer_name_mapping = {\n",
        "#             '3': \"relu1_2\",\n",
        "#             '8': \"relu2_2\",\n",
        "#             '15': \"relu3_3\"\n",
        "#         }\n",
        "\n",
        "#     def output_features(self, x):\n",
        "#         output = {}\n",
        "#         for name, module in self.vgg_layers._modules.items():\n",
        "#             x = module(x)\n",
        "#             if name in self.layer_name_mapping:\n",
        "#                 output[self.layer_name_mapping[name]] = x\n",
        "#         return list(output.values())\n",
        "\n",
        "#     def forward(self, dehaze, gt):\n",
        "#         loss = []\n",
        "#         dehaze_features = self.output_features(dehaze)\n",
        "#         gt_features = self.output_features(gt)\n",
        "#         for dehaze_feature, gt_feature in zip(dehaze_features, gt_features):\n",
        "#             loss.append(F.mse_loss(dehaze_feature, gt_feature))\n",
        "\n",
        "#         return sum(loss)/len(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZIBPJrelM8R"
      },
      "source": [
        "# # --- Define the perceptual loss network --- #\n",
        "# vgg_model = vgg16(pretrained=True).features[:16]\n",
        "# vgg_model = vgg_model.cuda()\n",
        "# for param in vgg_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "# loss_network = LossNetwork(vgg_model)\n",
        "# loss_network.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azUEv6TYCqy2"
      },
      "source": [
        "# from thop import profile\n",
        "# from thop import clever_format\n",
        "# import torchsummary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYaUcNw5F44F"
      },
      "source": [
        "# input = torch.randn(1, 4, 224, 224)\n",
        "# macs, params = profile(net, inputs=(input, ))\n",
        "\n",
        "# del input\n",
        "# macs, params = clever_format([macs, params], \"%.3f\")\n",
        "# print('MACs: {}', macs)\n",
        "# print('Params: {}', params)\n",
        "# torchsummary(net, input_size=(4, 224, 224))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF2TBT9f_ssk"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cePOjqKA6to4"
      },
      "source": [
        "# def quat_l2(output, target):\n",
        "#     check_input(output)\n",
        "#     check_input(target)\n",
        "#     r = get_r(output) - get_r(target)\n",
        "#     i = get_i(output) - get_i(target)\n",
        "#     j = get_j(output) - get_j(target)\n",
        "#     k = get_k(output) - get_k(target)\n",
        "#     modulus = torch.sqrt(r*r + i*i + j*j + k*k)\n",
        "#     loss = torch.mean(modulus)\n",
        "#     return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MZ_PGO5_uGv",
        "outputId": "66fde2d0-bc5b-4338-8410-c98b620261c0"
      },
      "source": [
        "opt = {'network': 'GCANet',\n",
        "          'name': 'default_exp',\n",
        "          'gpu_ids': '0',\n",
        "          'epochs': 50,\n",
        "          'lr': 0.001,\n",
        "          'lr_step': 40,\n",
        "          'lr_gamma': 0.1,\n",
        "          'weight_decay': 0.0005,\n",
        "          'checkpoints_dir': 'checkpoint',\n",
        "          'logDir': 'tblogdir',\n",
        "          'resume_dir': '',\n",
        "          'resume_epoch': 0,\n",
        "          'save_epoch': 5,\n",
        "          'save_latest_freq': 5000,\n",
        "          'test_epoch': 5,\n",
        "          'test_max_size': 1080,\n",
        "          'size_unit': 8,\n",
        "          'print_iter': 100,\n",
        "          'input_folder': 'OTS/hazy',\n",
        "          'gt_folder': 'OTS/gt',\n",
        "          'test_input_folder': 'SOTS/outdoor/hazy',\n",
        "          'test_gt_folder': 'SOTS/outdoor/gt',\n",
        "          'num_workers': 4,\n",
        "          'batch_size': 4,\n",
        "          'only_residual': False,\n",
        "          'loss_func': 'l2',\n",
        "          'inc': 4,\n",
        "          'outc': 4,\n",
        "          'force_rgb': 'store_true',\n",
        "          'no_edge':'store_true'}\n",
        "\n",
        "\n",
        "opt['input_folder'] = os.path.expanduser(opt['input_folder'])\n",
        "opt['gt_folder'] = os.path.expanduser(opt['gt_folder'])\n",
        "opt['test_input_folder'] = os.path.expanduser(opt['test_input_folder'])\n",
        "opt['test_gt_folder'] = os.path.expanduser(opt['test_gt_folder'])\n",
        "\n",
        "\n",
        "if not os.path.exists(os.path.join(opt['checkpoints_dir'], opt['name'])):\n",
        "    os.makedirs(os.path.join(opt['checkpoints_dir'], opt['name']))\n",
        "opt['resume_dir'] = opt['resume_dir'] if opt['resume_dir'] != '' else os.path.join(opt['checkpoints_dir'], opt['name'])\n",
        "\n",
        "# visualizer = TFVisualizer(opt)\n",
        "# ### Log out\n",
        "# with open(os.path.realpath(__file__), 'r') as fid:\n",
        "#     visualizer.print_logs(fid.read())\n",
        "\n",
        "# ## print argument\n",
        "# for key, val in vars(opt).items():\n",
        "#     visualizer.print_logs('%s: %s' % (key, val))\n",
        "\n",
        "opt['gpu_ids'] = [int(x) for x in opt['gpu_ids'].split(',')]\n",
        "assert all(0 <= x <= torch.cuda.device_count() for x in opt['gpu_ids']), 'gpu id should ' \\\n",
        "                                                      'be 0~{0}'.format(torch.cuda.device_count())\n",
        "torch.cuda.set_device(opt['gpu_ids'][0])\n",
        "\n",
        "\n",
        "train_dataset = ImagePairPrefixFolder(opt['input_folder'], opt['gt_folder'], size_unit=opt['size_unit'], force_rgb=opt['force_rgb'])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=opt['batch_size'], shuffle=True,\n",
        "                              collate_fn=var_custom_collate, pin_memory=True,\n",
        "                              num_workers=opt['num_workers'])\n",
        "\n",
        "opt['do_test'] = opt['test_gt_folder'] != ''\n",
        "if opt['do_test']:\n",
        "    test_dataset = ImagePairPrefixFolder(opt['test_input_folder'], \n",
        "                                         opt['test_gt_folder'],\n",
        "                                         max_img_size=opt['test_max_size'], \n",
        "                                         size_unit=opt['size_unit'], \n",
        "                                         force_rgb=opt['force_rgb'])\n",
        "    test_dataloader = DataLoader(test_dataset, \n",
        "                                 batch_size=1, \n",
        "                                 shuffle=False,\n",
        "                                 collate_fn=var_custom_collate, \n",
        "                                 pin_memory=True,\n",
        "                                 num_workers=1)\n",
        "\n",
        "total_inc = opt['inc'] if opt['no_edge'] else opt['inc'] + 1\n",
        "if opt['network'] == 'GCANet':\n",
        "    net = GCANet(in_c=4, out_c=4, only_residual=opt['only_residual'])\n",
        "else:\n",
        "    print('network structure %s not supported' % opt['network'])\n",
        "    raise ValueError\n",
        "\n",
        "#loss_crit = quat_l2\n",
        "loss_crit2 = torch.nn.MSELoss()\n",
        "loss_crit3 = torch.nn.L1Loss()\n",
        "pnsr_crit = torch.nn.MSELoss()\n",
        "\n",
        "if len(opt['gpu_ids']) > 0:\n",
        "    net.cuda()\n",
        "    if len(opt['gpu_ids']) > 1:\n",
        "        net = torch.nn.DataParallel(net)\n",
        "    pnsr_crit = pnsr_crit.cuda()\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=opt['lr'])\n",
        "step_optim_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=opt['lr_step'], gamma=opt['lr_gamma'])\n",
        "loss_avg = MovingAvg(pool_size=50)\n",
        "\n",
        "start_epoch = 0\n",
        "total_iter = 0\n",
        "\n",
        "if os.path.exists(os.path.join(opt['checkpoints_dir'], opt['name'], 'latest.pth')):\n",
        "    print('resuming from latest.pth')\n",
        "    latest_info = torch.load(os.path.join(opt['checkpoints_dir'], opt['name'], 'latest.pth'))\n",
        "    start_epoch = latest_info['epoch']\n",
        "    total_iter = latest_info['total_iter']\n",
        "    if isinstance(net, torch.nn.DataParallel):\n",
        "        net.module.load_state_dict(latest_info['net_state'])\n",
        "    else:\n",
        "        net.load_state_dict(latest_info['net_state'])\n",
        "    optimizer.load_state_dict(latest_info['optim_state'])\n",
        "\n",
        "if opt['resume_epoch'] > 0:\n",
        "    start_epoch = opt['resume_epoch']\n",
        "    total_iter = opt['resume_epoch'] * len(train_dataloader)\n",
        "    resume_path = os.path.join(opt['resume_epoch'], 'net_epoch_%d.pth') % opt['resume_epoch']\n",
        "    print('resume from : %s' % resume_path)\n",
        "    assert os.path.exists(resume_path), 'cannot find the resume model: %s ' % resume_path\n",
        "    if isinstance(net, torch.nn.DataParallel):\n",
        "        net.module.load_state_dict(torch.load(resume_path))\n",
        "    else:\n",
        "        net.load_state_dict(torch.load(resume_path))\n",
        "\n",
        "torchsummary.summary(net, input_size=(4, 224, 224))\n",
        "\n",
        "\n",
        "for epoch in range(start_epoch, opt['epochs']):\n",
        "    #visualizer.print_logs(\"Start to train epoch %d\" % epoch)\n",
        "    print(\"Start to train epoch %d\" % epoch)\n",
        "    net.train()\n",
        "    for iter, data in enumerate(train_dataloader):\n",
        "        total_iter += 1\n",
        "        optimizer.zero_grad()\n",
        "        step_optim_scheduler.step(epoch)\n",
        "\n",
        "        batch_input_img, batch_input_edge,  batch_gt = data\n",
        "        if len(opt['gpu_ids']) > 0:\n",
        "            batch_input_img, batch_input_edge, batch_gt = batch_input_img.cuda(), batch_input_edge.cuda(), batch_gt.cuda()\n",
        "\n",
        "        if opt['no_edge']:\n",
        "            batch_input = batch_input_img\n",
        "        else:\n",
        "            batch_input = torch.cat((batch_input_img, batch_input_edge), dim=1)\n",
        "   \n",
        "        batch_input_v = Variable(batch_input)\n",
        "        if opt['only_residual']:\n",
        "            batch_gt_v = Variable(batch_gt - (batch_input_img+128))\n",
        "        else:\n",
        "            batch_gt_v = Variable(batch_gt)\n",
        "\n",
        "        pred = net(batch_input_v)\n",
        "\n",
        "        #rec_loss1 = loss_crit(pred, batch_gt_v)\n",
        "        rec_loss2 = loss_crit2(pred, batch_gt_v)\n",
        "        rec_loss3 = loss_crit3(pred, batch_gt_v)\n",
        "        #perceptual_loss = loss_network(pred[:, 1:, :, :], batch_gt_v[:, 1:, :, :])\n",
        "        loss = (rec_loss2)*0.5 + (rec_loss3)*0.5 # + 0.04 *perceptual_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss = loss_avg.set_curr_val(loss.data)\n",
        "\n",
        "        if iter % opt['print_iter'] == 0:\n",
        "            # visualizer.plot_current_losses(total_iter, { 'loss': loss})\n",
        "            # visualizer.print_logs('%s Step[%d/%d], lr: %f, mv_avg_loss: %f, loss: %f' %\n",
        "            #                         (str(datetime.datetime.now()).split(' ')[1], iter, len(train_dataloader),\n",
        "            #                          step_optim_scheduler.get_lr()[0], avg_loss, loss))\n",
        "            pass\n",
        "\n",
        "        if total_iter % opt['save_latest_freq'] == 0:\n",
        "            latest_info = {'total_iter': total_iter,\n",
        "                           'epoch': epoch,\n",
        "                           'optim_state': optimizer.state_dict()}\n",
        "            if len(opt['gpu_ids']) > 1:\n",
        "                latest_info['net_state'] = net.module.state_dict()\n",
        "            else:\n",
        "                latest_info['net_state'] = net.state_dict()\n",
        "            print('save lastest model.')\n",
        "            torch.save(latest_info, os.path.join(opt['checkpoints_dir'], opt['name'], 'latest.pth'))\n",
        "\n",
        "    if (epoch+1) % opt['save_epoch'] == 0 :\n",
        "        #visualizer.print_logs('saving model for epoch %d' % epoch)\n",
        "        if len(opt['gpu_ids']) > 1:\n",
        "            torch.save(net.module.state_dict(), os.path.join(opt['checkpoints_dir'], opt['name'], 'net_epoch_%d.pth' % (epoch+1)))\n",
        "        else:\n",
        "            torch.save(net.state_dict(), os.path.join(opt['checkpoints_dir'], opt['name'], 'net_epoch_%d.pth' % (epoch + 1)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           2,304\n",
            "    InstanceNorm2d-2         [-1, 64, 224, 224]             128\n",
            "            Conv2d-3         [-1, 64, 224, 224]          36,864\n",
            "    InstanceNorm2d-4         [-1, 64, 224, 224]             128\n",
            "            Conv2d-5         [-1, 64, 112, 112]          36,864\n",
            "    InstanceNorm2d-6         [-1, 64, 112, 112]             128\n",
            "      ShareSepConv-7         [-1, 64, 112, 112]               9\n",
            "            Conv2d-8         [-1, 64, 112, 112]          36,864\n",
            "    InstanceNorm2d-9         [-1, 64, 112, 112]             128\n",
            "     ShareSepConv-10         [-1, 64, 112, 112]               9\n",
            "           Conv2d-11         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-12         [-1, 64, 112, 112]             128\n",
            "SmoothDilatedResidualBlock-13         [-1, 64, 112, 112]               0\n",
            "     ShareSepConv-14         [-1, 64, 112, 112]               9\n",
            "           Conv2d-15         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-16         [-1, 64, 112, 112]             128\n",
            "     ShareSepConv-17         [-1, 64, 112, 112]               9\n",
            "           Conv2d-18         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-19         [-1, 64, 112, 112]             128\n",
            "SmoothDilatedResidualBlock-20         [-1, 64, 112, 112]               0\n",
            "     ShareSepConv-21         [-1, 64, 112, 112]               9\n",
            "           Conv2d-22         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-23         [-1, 64, 112, 112]             128\n",
            "     ShareSepConv-24         [-1, 64, 112, 112]               9\n",
            "           Conv2d-25         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-26         [-1, 64, 112, 112]             128\n",
            "SmoothDilatedResidualBlock-27         [-1, 64, 112, 112]               0\n",
            "     ShareSepConv-28         [-1, 64, 112, 112]              49\n",
            "           Conv2d-29         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-30         [-1, 64, 112, 112]             128\n",
            "     ShareSepConv-31         [-1, 64, 112, 112]              49\n",
            "           Conv2d-32         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-33         [-1, 64, 112, 112]             128\n",
            "SmoothDilatedResidualBlock-34         [-1, 64, 112, 112]               0\n",
            "     ShareSepConv-35         [-1, 64, 112, 112]              49\n",
            "           Conv2d-36         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-37         [-1, 64, 112, 112]             128\n",
            "     ShareSepConv-38         [-1, 64, 112, 112]              49\n",
            "           Conv2d-39         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-40         [-1, 64, 112, 112]             128\n",
            "SmoothDilatedResidualBlock-41         [-1, 64, 112, 112]               0\n",
            "     ShareSepConv-42         [-1, 64, 112, 112]              49\n",
            "           Conv2d-43         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-44         [-1, 64, 112, 112]             128\n",
            "     ShareSepConv-45         [-1, 64, 112, 112]              49\n",
            "           Conv2d-46         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-47         [-1, 64, 112, 112]             128\n",
            "SmoothDilatedResidualBlock-48         [-1, 64, 112, 112]               0\n",
            "           Conv2d-49         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-50         [-1, 64, 112, 112]             128\n",
            "           Conv2d-51         [-1, 64, 112, 112]          36,864\n",
            "   InstanceNorm2d-52         [-1, 64, 112, 112]             128\n",
            "    ResidualBlock-53         [-1, 64, 112, 112]               0\n",
            "           Conv2d-54         [-1, 64, 114, 114]          12,352\n",
            "  ConvTranspose2d-55         [-1, 64, 224, 224]          65,600\n",
            "   InstanceNorm2d-56         [-1, 64, 224, 224]             128\n",
            "  ConvTranspose2d-57         [-1, 64, 224, 224]          73,792\n",
            "   InstanceNorm2d-58         [-1, 64, 224, 224]             128\n",
            "           Conv2d-59          [-1, 4, 224, 224]             260\n",
            "================================================================\n",
            "Total params: 746,912\n",
            "Trainable params: 746,912\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.77\n",
            "Forward/backward pass size (MB): 504.00\n",
            "Params size (MB): 2.85\n",
            "Estimated Total Size (MB): 507.62\n",
            "----------------------------------------------------------------\n",
            "Start to train epoch 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:154: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
            "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "save lastest model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SmHyj7hdMNC"
      },
      "source": [
        "NAME = 'final_retrain_real'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ify41FCDnHhs"
      },
      "source": [
        "def restore_image(fname):\n",
        "  img = Image.open(fname).convert('RGB')\n",
        "  im_w, im_h = img.size\n",
        "  if im_w % 4 != 0 or im_h % 4 != 0:\n",
        "      img = img.resize((int(im_w // 4 * 4), int(im_h // 4 * 4))) \n",
        "  #img = np.array(img).astype('float')\n",
        "\n",
        "  img = np.array(img)\n",
        "  h, w, c = img.shape\n",
        "  #gray = cv2.imread(fname.replace('hazy', 'trans').replace('.jpg','.png'), 0)\n",
        "  #gray = cv2.resize(gray, (w, h))\n",
        "\n",
        "  gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "  img = np.dstack([gray[:, :, np.newaxis], img])\n",
        "  img = img.astype('float')\n",
        "\n",
        "  img_data = torch.from_numpy(img.transpose((2, 0, 1))).float()\n",
        "  c, w, h = img_data.size()\n",
        "  #edge_data = torch.zeros((1, w, h))\n",
        "  #in_data = torch.cat((img_data, edge_data), dim=0)\n",
        "  #in_data = torch.cat((img_data, edge_data), dim=1).unsqueeze(0) - 128 \n",
        "  in_data = img_data.reshape(1, 4, w, h) - 128\n",
        "  in_data = in_data.cuda()\n",
        "  with torch.no_grad():\n",
        "      pred = net(Variable(in_data))\n",
        "\n",
        "  out_img_data = (pred.data[0].cpu().float()).round().clamp(0, 255)\n",
        "  out_img = out_img_data.numpy().astype(np.uint8).transpose(1, 2, 0)\n",
        "  return out_img[:, :, 1:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1NJ6iZHJJpP"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from skimage.metrics import peak_signal_noise_ratio\n",
        "from skimage.metrics import structural_similarity\n",
        "\n",
        "def image_colorfulness(image):\n",
        "\t# split the image into its respective RGB components\n",
        "\t(B, G, R) = cv2.split(image.astype(\"float\"))\n",
        "\t# compute rg = R - G\n",
        "\trg = np.absolute(R - G)\n",
        "\t# compute yb = 0.5 * (R + G) - B\n",
        "\tyb = np.absolute(0.5 * (R + G) - B)\n",
        "\t# compute the mean and standard deviation of both `rg` and `yb`\n",
        "\t(rbMean, rbStd) = (np.mean(rg), np.std(rg))\n",
        "\t(ybMean, ybStd) = (np.mean(yb), np.std(yb))\n",
        "\t# combine the mean and standard deviations\n",
        "\tstdRoot = np.sqrt((rbStd ** 2) + (ybStd ** 2))\n",
        "\tmeanRoot = np.sqrt((rbMean ** 2) + (ybMean ** 2))\n",
        "\t# derive the \"colorfulness\" metric and return it\n",
        "\treturn stdRoot + (0.3 * meanRoot)\n",
        "\n",
        "\n",
        "hazy_dir = 'SOTS/outdoor/hazy/'\n",
        "gt_dir = 'SOTS/outdoor/gt/'\n",
        "hazy_imgs = [os.path.basename(fn) for fn in glob(os.path.join(hazy_dir, '*.jpg'))]\n",
        "\n",
        "\n",
        "psnrs = []\n",
        "ssims = []\n",
        "!mkdir /content/drive/MyDrive/processed/{NAME}\n",
        "for name in hazy_imgs:\n",
        "  fname = os.path.basename(name).split(\"_\")[0] + '.png'\n",
        "  image_name = os.path.join(hazy_dir, name)\n",
        "  gt_name = os.path.join(gt_dir, fname)\n",
        "\n",
        "  reconstructed = restore_image(image_name)\n",
        "  height, width = reconstructed.shape[:2]\n",
        "  gt = cv2.imread(gt_name)\n",
        "  gt = cv2.resize(gt, (width, height))\n",
        "  gt = cv2.cvtColor(gt, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  bgr = cv2.cvtColor(reconstructed, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "  cv2.imwrite(os.path.join('/content/drive/MyDrive/processed/{}'.format(NAME), name.replace('.jpg', '.png')), bgr)\n",
        "\n",
        "  psnrs.append(peak_signal_noise_ratio(gt, reconstructed))\n",
        "\n",
        "  img1 = cv2.cvtColor(reconstructed, cv2.COLOR_RGB2GRAY)\n",
        "  img2 = cv2.cvtColor(gt, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "  ssims.append(structural_similarity(img2, img1))\n",
        "\n",
        "print('ssim:', sum(ssims)/len(ssims))\n",
        "print('psnr:', sum(psnrs)/len(psnrs))\n",
        "\n",
        "!mkdir /content/drive/MyDrive/networks/{NAME}\n",
        "!cp -r checkpoint /content/drive/MyDrive/networks/{NAME}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OJNE5HejDbi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}